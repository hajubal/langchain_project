{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a898547",
   "metadata": {},
   "source": [
    "## 1. 환경 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e15f76c",
   "metadata": {},
   "source": [
    "`(1) Env 환경변수`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6d77f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0800c924",
   "metadata": {},
   "source": [
    "`(2) 기본 라이브러리`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bca641c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "from pprint import pprint\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee0237e",
   "metadata": {},
   "source": [
    "## 2. 다양한 문서 형식 처리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26db4266",
   "metadata": {},
   "source": [
    "### 2.1 PDF 문서 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6650c629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_loader = PyPDFLoader('./data/transformer.pdf')\n",
    "pdf_docs = pdf_loader.load()\n",
    "\n",
    "len(pdf_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d92305",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pdf_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d23dd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a414b9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pdf_docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d394f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pdf_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6178f9a0",
   "metadata": {},
   "source": [
    "### 2.2 웹 문서 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8732bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "web_loader = WebBaseLoader([\"https://python.langchain.com/\", \"https://js.langchain.com/\"])\n",
    "\n",
    "web_docs = web_loader.load()\n",
    "\n",
    "len(web_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64688c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451fda18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(web_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b473046",
   "metadata": {},
   "source": [
    "### 2.3 JSON 파일 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125d323f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "json_loader = JSONLoader(\n",
    "    file_path=\"./data/kakao_chat.json\",\n",
    "    jq_schema=\".messages[].content\",    # messages 배열의 content 필드만 추출\n",
    "    text_content=True,                  # 추출하려는 필드가 텍스트인지 여부\n",
    ")\n",
    "\n",
    "json_docs = json_loader.load()\n",
    "\n",
    "print(\"문서의 수:\", len(json_docs))\n",
    "print(\"-\" * 50)\n",
    "print(\"처음 문서의 메타데이터: \\n\", json_docs[0].metadata)\n",
    "print(\"-\" * 50)\n",
    "print(\"처음 문서의 내용: \\n\", json_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6004a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "json_loader = JSONLoader(\n",
    "    file_path=\"./data/kakao_chat.json\",\n",
    "    jq_schema=\".messages[]\",    # messages 배열의 모든 아이템을 추출\n",
    "    text_content=False,          # 추출하려는 필드가 텍스트인지 여부\n",
    ")\n",
    "\n",
    "json_docs = json_loader.load()\n",
    "\n",
    "print(\"문서의 수:\", len(json_docs))\n",
    "print(\"-\" * 50)\n",
    "print(\"처음 문서의 메타데이터: \\n\", json_docs[0].metadata)\n",
    "print(\"-\" * 50)\n",
    "print(\"처음 문서의 내용: \\n\", json_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eafd1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유니코드 디코딩 (한글 문자들이 유니코드 이스케이프 시퀀스로 인코딩되어 있음)\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "decoded_json_docs = []\n",
    "for doc in json_docs:\n",
    "\n",
    "    decoded_data = json.loads(doc.page_content)\n",
    "\n",
    "    # decoded_json_docs.append({\n",
    "    #     \"metadata\": doc.metadata,\n",
    "    #     \"page_content\": json.dumps(decoded_data, ensure_ascii=False)\n",
    "    # })\n",
    "\n",
    "    document_obj = Document(page_content=json.dumps(decoded_data, ensure_ascii=False), metadata=doc.metadata)\n",
    "    decoded_json_docs.append(document_obj)\n",
    "\n",
    "print(\"문서의 수:\", len(decoded_json_docs))\n",
    "print(\"-\" * 50)\n",
    "# print(\"처음 문서의 메타데이터: \\n\", decoded_json_docs[0][\"metadata\"])\n",
    "print(\"처음 문서의 메타데이터: \\n\", decoded_json_docs[0].metadata)\n",
    "print(\"-\" * 50)\n",
    "# print(\"처음 문서의 내용: \\n\", decoded_json_docs[0][\"page_content\"])\n",
    "print(\"처음 문서의 내용: \\n\", decoded_json_docs[0].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c112cbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메타데이터 추가하기\n",
    "def metadata_func(record: dict, metadata: dict) -> dict:\n",
    "    metadata[\"sender\"] = record.get(\"sender\")\n",
    "    metadata[\"timestamp\"] = record.get(\"timestamp\")\n",
    "    return metadata\n",
    "\n",
    "\n",
    "json_loader = JSONLoader(\n",
    "    file_path=\"./data/kakao_chat.json\",\n",
    "    jq_schema=\".messages[]\",\n",
    "    content_key=\"content\",\n",
    "    metadata_func=metadata_func,\n",
    ")\n",
    "\n",
    "json_docs = json_loader.load()\n",
    "\n",
    "print(\"문서의 수:\", len(json_docs))\n",
    "print(\"-\" * 50)\n",
    "print(\"처음 문서의 메타데이터: \\n\", json_docs[0].metadata)\n",
    "print(\"-\" * 50)\n",
    "print(\"처음 문서의 내용: \\n\", json_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0d5ede",
   "metadata": {},
   "source": [
    "```json\n",
    "{\"sender\": \"김철수\", \"timestamp\": \"2023-09-15 09:30:22\", \"content\": \"안녕하세요 여러분, 오늘 회의 시간 확인차 연락드립니다.\"}\n",
    "{\"sender\": \"이영희\", \"timestamp\": \"2023-09-15 09:31:05\", \"content\": \"네, 안녕하세요. 오후 2시에 하기로 했어요.\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32253193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSONL 파일 로드하기\n",
    "json_loader = JSONLoader(\n",
    "    file_path=\"./data/kakao_chat.jsonl\",\n",
    "    jq_schema=\".content\",\n",
    "    json_lines=True,\n",
    ")\n",
    "\n",
    "json_docs = json_loader.load()\n",
    "\n",
    "print(\"문서의 수:\", len(json_docs))\n",
    "print(\"-\" * 50)\n",
    "print(\"처음 문서의 메타데이터: \\n\", json_docs[0].metadata)\n",
    "print(\"-\" * 50)\n",
    "print(\"처음 문서의 내용: \\n\", json_docs[0].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9335e1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSONL 파일 로드하기\n",
    "json_loader = JSONLoader(\n",
    "    file_path=\"./data/kakao_chat.jsonl\",\n",
    "    jq_schema=\".\",\n",
    "    content_key=\"content\",\n",
    "    json_lines=True,\n",
    ")\n",
    "\n",
    "json_docs = json_loader.load()\n",
    "\n",
    "print(\"문서의 수:\", len(json_docs))\n",
    "print(\"-\" * 50)\n",
    "print(\"처음 문서의 메타데이터: \\n\", json_docs[0].metadata)\n",
    "print(\"-\" * 50)\n",
    "print(\"처음 문서의 내용: \\n\", json_docs[0].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255e4ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메타데이터 추가하기\n",
    "def metadata_func(record: dict, metadata: dict) -> dict:\n",
    "    metadata[\"sender\"] = record.get(\"sender\")\n",
    "    metadata[\"timestamp\"] = record.get(\"timestamp\")\n",
    "    return metadata\n",
    "\n",
    "json_loader = JSONLoader(\n",
    "    file_path=\"./data/kakao_chat.jsonl\",\n",
    "    jq_schema=\".\",\n",
    "    content_key=\"content\",\n",
    "    metadata_func=metadata_func,\n",
    "    json_lines=True,\n",
    ")\n",
    "\n",
    "json_docs = json_loader.load()\n",
    "\n",
    "print(\"문서의 수:\", len(json_docs))\n",
    "print(\"-\" * 50)\n",
    "print(\"처음 문서의 메타데이터: \\n\", json_docs[0].metadata)\n",
    "print(\"-\" * 50)\n",
    "print(\"처음 문서의 내용: \\n\", json_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ae07e1",
   "metadata": {},
   "source": [
    "### 2.4  CSV 문서 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7a8b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "csv_loader = CSVLoader(\"./data/kbo_teams_2023.csv\")\n",
    "csv_docs = csv_loader.load()\n",
    "\n",
    "print(\"문서의 수:\", len(csv_docs))\n",
    "print(\"-\" * 50)\n",
    "print(\"처음 문서의 메타데이터: \\n\", csv_docs[0].metadata)\n",
    "print(\"-\" * 50)\n",
    "print(\"처음 문서의 내용: \\n\", csv_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aae49f4",
   "metadata": {},
   "source": [
    "## 3. 효과적인 텍스트 분할 전략"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7e0cf4",
   "metadata": {},
   "source": [
    "### 3-1 RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc4bc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,        # 청크 크기  \n",
    "    chunk_overlap=200,      # 청크 중 중복되는 부분 크기\n",
    "    length_function=len,    # 글자 수를 기준으로 분할\n",
    "    separators=[\"\\n\\n\", \"\\n\",],  # 구분자\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_documents(pdf_docs)\n",
    "print(f\"생성된 텍스트 청크 수: {len(texts)}\")\n",
    "print(f\"각 청크의 길이: {list(len(text.page_content) for text in texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee0ca46",
   "metadata": {},
   "source": [
    "- RecursiveCharacterTextSplitter는 이름에서 알 수 있듯이 재귀적으로 텍스트를 분할합니다. \n",
    "- 구분자를 순차적으로 적용하여 큰 청크에서 시작하여 점진적으로 더 작은 단위로 나눕니다. \n",
    "- 일반적으로 CharacterTextSplitter보다 더 엄격하게 크기를 준수하려고 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714ba916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 청크의 시작 부분과 끝 부분 확인\n",
    "for text in texts[:5]:\n",
    "    print(text.page_content[:200])\n",
    "    print(\"-\" * 200)\n",
    "    print(text.page_content[-200:])\n",
    "    print(\"=\" * 200)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae8677d",
   "metadata": {},
   "source": [
    "### 3-2 정규표현식 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e6e771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장을 구분하여 분할 (마침표, 느낌표, 물음표 다음에 공백이 오는 경우 문장의 끝으로 판단)\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=10,\n",
    "    chunk_overlap=0,\n",
    "    separator=r'(?<=[.!?])\\s+',\n",
    "    is_separator_regex=True,\n",
    "    keep_separator=True,\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_documents(json_docs)\n",
    "print(f\"생성된 텍스트 청크 수: {len(texts)}\")\n",
    "print(f\"각 청크의 길이: {list(len(text.page_content) for text in texts)}\")\n",
    "print()\n",
    "\n",
    "# 각 청크의 시작 부분과 끝 부분 확인\n",
    "for text in texts[:5]:\n",
    "    print(text.page_content[:50])\n",
    "    print(\"-\" * 50)\n",
    "    print(text.page_content[-50:])\n",
    "    print(\"=\" * 50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbe562f",
   "metadata": {},
   "source": [
    "### 3-3 토큰 수를 기반으로 분할"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9198b73",
   "metadata": {},
   "source": [
    "`(1) tiktoken`  \n",
    "- OpenAI에서 만든 BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdea518c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pdf_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b5fda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"cl100k_base\", \n",
    "    # model_name=\"gpt-4o-mini\",\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=0,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(pdf_docs[:1])\n",
    "\n",
    "print(f\"생성된 청크 수: {len(chunks)}\")\n",
    "print(f\"각 청크의 길이: {list(len(chunk.page_content) for chunk in chunks)}\")\n",
    "\n",
    "# 각 청크의 시작 부분과 끝 부분 확인\n",
    "for chunk in chunks[:5]:\n",
    "    print(chunk.page_content[:50])\n",
    "    print(\"-\" * 50)\n",
    "    print(chunk.page_content[-50:])\n",
    "    print(\"=\" * 50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10e8b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "# tokenizer = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "for chunk in chunks[:5]:\n",
    "\n",
    "    # 각 청크를 토큰화\n",
    "    tokens = tokenizer.encode(chunk.page_content)\n",
    "    # 각 청크의 단어 수 확인\n",
    "    print(len(tokens))\n",
    "    # 각 청크의 토큰화 결과 확인 (첫 10개 토큰만 출력)\n",
    "    print(tokens[:10])\n",
    "    # 토큰 ID를 실제 토큰(문자열)로 변환해서 출력\n",
    "    token_strings = [tokenizer.decode([token]) for token in tokens[:10]]\n",
    "    print(token_strings)\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b01677",
   "metadata": {},
   "source": [
    "`(2) Hugging Face 토크나이저`  \n",
    "- Hugging Face tokenizer 모델이 토큰 수를 기준으로 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6872bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-m3\")\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1477bfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 인코딩\n",
    "tokens = tokenizer.encode(\"안녕하세요. 반갑습니다.\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07cafbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰을 출력\n",
    "print(tokenizer.convert_ids_to_tokens(tokens)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcf6a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코딩\n",
    "print(tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311dec97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer=tokenizer,\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=0,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(pdf_docs[:1])\n",
    "\n",
    "print(f\"생성된 청크 수: {len(chunks)}\")\n",
    "print(f\"각 청크의 길이: {list(len(chunk.page_content) for chunk in chunks)}\")\n",
    "print()\n",
    "\n",
    "for chunk in chunks[:5]:\n",
    "\n",
    "    # 각 청크를 토큰화\n",
    "    tokens = tokenizer.encode(chunk.page_content)\n",
    "    # 각 청크의 단어 수 확인\n",
    "    print(len(tokens))\n",
    "    # 각 청크의 토큰화 결과 확인 (첫 10개 토큰만 출력)\n",
    "    print(tokens[:10])\n",
    "    # 토큰 ID를 실제 토큰(문자열)로 변환해서 출력\n",
    "    token_strings = tokenizer.convert_ids_to_tokens(tokens[:10]) \n",
    "    print(token_strings)\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc751ee",
   "metadata": {},
   "source": [
    "### 3-4 Semantic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "043984b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# 임베딩 모델을 사용하여 SemanticChunker를 초기화 \n",
    "text_splitter = SemanticChunker(\n",
    "    embeddings=OpenAIEmbeddings(),         # OpenAI 임베딩 사용\n",
    "    breakpoint_threshold_type=\"gradient\",  # 기준점 타입 설정 (gradient, percentile, standard_deviation, interquartile)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e067dc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.split_documents(pdf_docs[:1])\n",
    "\n",
    "print(f\"생성된 청크 수: {len(chunks)}\")\n",
    "print(f\"각 청크의 길이: {list(len(chunk.page_content) for chunk in chunks)}\")\n",
    "print()\n",
    "\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "\n",
    "for chunk in chunks[:5]:\n",
    "\n",
    "    # 각 청크를 토큰화\n",
    "\n",
    "    tokens = tokenizer.encode(chunk.page_content)\n",
    "    # 각 청크의 단어 수 확인\n",
    "    print(len(tokens))\n",
    "    # 각 청크의 내용을 확인\n",
    "    print(chunk.page_content[:100])\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc16413",
   "metadata": {},
   "source": [
    "## 4. 문서 임베딩(Embedding) 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315e28ef",
   "metadata": {},
   "source": [
    "### 4-1 OpenAI "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d905110a",
   "metadata": {},
   "source": [
    "`(1) embedding 모델`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617a2cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# OpenAIEmbeddings 모델 생성\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# 임베딩 객체 출력\n",
    "embeddings_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3123b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 모델의 컨텍스트 길이 확인\n",
    "embeddings_model.embedding_ctx_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac45eaf",
   "metadata": {},
   "source": [
    "`(2) embed_documents 사용`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb43129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 컬렉션\n",
    "documents = [\n",
    "    \"인공지능은 컴퓨터 과학의 한 분야입니다.\",\n",
    "    \"머신러닝은 인공지능의 하위 분야입니다.\",\n",
    "    \"딥러닝은 머신러닝의 한 종류입니다.\",\n",
    "    \"자연어 처리는 컴퓨터가 인간의 언어를 이해하고 생성하는 기술입니다.\",\n",
    "    \"컴퓨터 비전은 컴퓨터가 디지털 이미지나 비디오를 이해하는 방법을 연구합니다.\"\n",
    "]\n",
    "\n",
    "# 문서 임베딩\n",
    "document_embeddings = embeddings_model.embed_documents(documents)\n",
    "\n",
    "# 임베딩 결과 출력\n",
    "print(f\"임베딩 벡터의 개수: {len(document_embeddings)}\")\n",
    "print(f\"임베딩 벡터의 차원: {len(document_embeddings[0])}\")\n",
    "print(document_embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef1d8f7",
   "metadata": {},
   "source": [
    "`(3) embed_query 사용`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2da83df",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_query = embeddings_model.embed_query(\"인공지능이란 무엇인가요?\")\n",
    "\n",
    "# 쿼리 임베딩 결과 출력\n",
    "print(f\"쿼리 임베딩 벡터의 차원: {len(embedded_query)}\")\n",
    "print(embedded_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6a23e3",
   "metadata": {},
   "source": [
    "`(4) 유사도 기반 검색`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c3435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utils.math import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# 쿼리와 가장 유사한 문서 찾기 함수\n",
    "def find_most_similar(query, doc_embeddings):\n",
    "    query_embedding = embeddings_model.embed_query(query)\n",
    "    # similarities = [cosine_similarity(query_embedding, doc_emb) for doc_emb in doc_embeddings]\n",
    "    similarities = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
    "    most_similar_idx = np.argmax(similarities)\n",
    "    return documents[most_similar_idx], similarities[most_similar_idx]\n",
    "\n",
    "# 예제 쿼리\n",
    "queries = [\n",
    "    \"인공지능이란 무엇인가요?\",\n",
    "    \"딥러닝과 머신러닝의 관계는 어떻게 되나요?\",\n",
    "    \"컴퓨터가 이미지를 이해하는 방법은?\"\n",
    "]\n",
    "\n",
    "# 각 쿼리에 대해 가장 유사한 문서 찾기\n",
    "for query in queries:\n",
    "    most_similar_doc, similarity = find_most_similar(query, document_embeddings)\n",
    "    print(f\"쿼리: {query}\")\n",
    "    print(f\"가장 유사한 문서: {most_similar_doc}\")\n",
    "    print(f\"유사도: {similarity:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fcfdce",
   "metadata": {},
   "source": [
    "### 4-2 Huggingface - 오픈소스 LLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1572d97",
   "metadata": {},
   "source": [
    "`(1) embedding 모델`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7af1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Hugging Face의 임베딩 모델 생성\n",
    "embeddings_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "# 임베딩 객체 출력\n",
    "embeddings_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cbe740",
   "metadata": {},
   "source": [
    "`(2) embed_documents 사용`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094a8891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 임베딩\n",
    "document_embeddings = embeddings_model.embed_documents(documents)\n",
    "\n",
    "# 임베딩 결과 출력\n",
    "print(f\"임베딩 벡터의 개수: {len(document_embeddings)}\")\n",
    "print(f\"임베딩 벡터의 차원: {len(document_embeddings[0])}\")\n",
    "print(document_embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ba092f",
   "metadata": {},
   "source": [
    "`(3) embed_query 사용`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3832ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_query = embeddings_model.embed_query(\"인공지능이란 무엇인가요?\")\n",
    "\n",
    "# 쿼리 임베딩 결과 출력\n",
    "print(f\"쿼리 임베딩 벡터의 차원: {len(embedded_query)}\")\n",
    "print(embedded_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e87fd17",
   "metadata": {},
   "source": [
    "`(4) 유사도 기반 검색`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76435ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제 쿼리\n",
    "queries = [\n",
    "    \"인공지능이란 무엇인가요?\",\n",
    "    \"딥러닝과 머신러닝의 관계는 어떻게 되나요?\",\n",
    "    \"컴퓨터가 이미지를 이해하는 방법은?\"\n",
    "]\n",
    "\n",
    "# 각 쿼리에 대해 가장 유사한 문서 찾기\n",
    "for query in queries:\n",
    "    most_similar_doc, similarity = find_most_similar(query, document_embeddings)\n",
    "    print(f\"쿼리: {query}\")\n",
    "    print(f\"가장 유사한 문서: {most_similar_doc}\")\n",
    "    print(f\"유사도: {similarity:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d063911",
   "metadata": {},
   "source": [
    "### 4-3 Ollama - 오픈소스 LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba69a0f5",
   "metadata": {},
   "source": [
    "`(1) embedding 모델`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795906a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# OllamaEmbeddings 모델 생성\n",
    "# embeddings_model = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "embeddings_model = OllamaEmbeddings(model=\"bge-m3\")\n",
    "\n",
    "# 임베딩 객체 출력\n",
    "embeddings_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0673debb",
   "metadata": {},
   "source": [
    "`(2) embed_documents 사용`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fde851f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 컬렉션\n",
    "documents = [\n",
    "    \"인공지능은 컴퓨터 과학의 한 분야입니다.\",\n",
    "    \"머신러닝은 인공지능의 하위 분야입니다.\",\n",
    "    \"딥러닝은 머신러닝의 한 종류입니다.\",\n",
    "    \"자연어 처리는 컴퓨터가 인간의 언어를 이해하고 생성하는 기술입니다.\",\n",
    "    \"컴퓨터 비전은 컴퓨터가 디지털 이미지나 비디오를 이해하는 방법을 연구합니다.\"\n",
    "]\n",
    "\n",
    "# 문서 임베딩\n",
    "document_embeddings = embeddings_model.embed_documents(documents)\n",
    "\n",
    "# 임베딩 결과 출력\n",
    "print(f\"임베딩 벡터의 개수: {len(document_embeddings)}\")\n",
    "print(f\"임베딩 벡터의 차원: {len(document_embeddings[0])}\")\n",
    "print(document_embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668cb852",
   "metadata": {},
   "source": [
    "`(3) embed_query 사용`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd5a102",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_query = embeddings_model.embed_query(\"인공지능이란 무엇인가요?\")\n",
    "\n",
    "# 쿼리 임베딩 결과 출력\n",
    "print(f\"쿼리 임베딩 벡터의 차원: {len(embedded_query)}\")\n",
    "print(embedded_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f5f115",
   "metadata": {},
   "source": [
    "`(4) 유사도 기반 검색`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c678c6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제 쿼리\n",
    "queries = [\n",
    "    \"인공지능이란 무엇인가요?\",\n",
    "    \"딥러닝과 머신러닝의 관계는 어떻게 되나요?\",\n",
    "    \"컴퓨터가 이미지를 이해하는 방법은?\"\n",
    "]\n",
    "\n",
    "# 각 쿼리에 대해 가장 유사한 문서 찾기\n",
    "for query in queries:\n",
    "    most_similar_doc, similarity = find_most_similar(query, document_embeddings)\n",
    "    print(f\"쿼리: {query}\")\n",
    "    print(f\"가장 유사한 문서: {most_similar_doc}\")\n",
    "    print(f\"유사도: {similarity:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d7177b",
   "metadata": {},
   "source": [
    "## 5. 다국어 RAG 시스템 구축 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0693d5b9",
   "metadata": {},
   "source": [
    "### 5-1 언어 교차(cross-lingual) 검색"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51c6bcf",
   "metadata": {},
   "source": [
    "`(1) 다국어 문서 로드 및 전처리` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26541439",
   "metadata": {},
   "outputs": [],
   "source": [
    "korean_txt_files = glob(os.path.join('data', '*_KR.txt')) \n",
    "english_txt_files = glob(os.path.join('data', '*_EN.txt'))\n",
    "\n",
    "print(\"한국어 텍스트 파일:\", korean_txt_files)\n",
    "print(\"영어 텍스트 파일:\", english_txt_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b74f19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "def load_text_files(txt_files):\n",
    "    data = []\n",
    "\n",
    "    for text_file in txt_files:\n",
    "        loader = TextLoader(text_file, encoding='utf-8')\n",
    "        data += loader.load()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "korean_data = load_text_files(korean_txt_files)\n",
    "english_data = load_text_files(english_txt_files)\n",
    "\n",
    "print(\"한국어 데이터 수:\", len(korean_data))\n",
    "print(\"영어 데이터 수:\", len(english_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bb6bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장을 구분하여 분할 (마침표, 느낌표, 물음표 다음에 공백이 오는 경우 문장의 끝으로 판단)\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    model_name=\"text-embedding-3-small\",\n",
    "    separator=r\"[.!?]\\s+\",\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=0,\n",
    "    is_separator_regex=True,\n",
    "    keep_separator=False,\n",
    ")\n",
    "\n",
    "korean_docs = text_splitter.split_documents(korean_data)\n",
    "english_docs = text_splitter.split_documents(english_data)\n",
    "\n",
    "print(\"한국어 문서 수:\", len(korean_docs))\n",
    "print(\"영어 문서 수:\", len(english_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d55f1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 청크의 시작 부분과 끝 부분 확인\n",
    "for doc in korean_docs[:2]:\n",
    "    print(doc.page_content[:50])\n",
    "    print(\"-\" * 50)\n",
    "    print(doc.page_content[-50:])\n",
    "    print(\"=\" * 50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9124c455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 청크의 시작 부분과 끝 부분 확인\n",
    "for doc in english_docs[:2]:\n",
    "    print(doc.page_content[:50])\n",
    "    print(\"-\" * 50)\n",
    "    print(doc.page_content[-50:])\n",
    "    print(\"=\" * 50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5b01db",
   "metadata": {},
   "source": [
    "`(2) 문서 임베딩 및 벡터저장소에 저장 `  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "abe717fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI 임베딩 모델 생성\n",
    "embeddings_openai = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Hugoing Face 임베딩 모델 생성\n",
    "embeddings_huggingface = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "# Ollama 임베딩 모델 생성\n",
    "embeddings_ollama = OllamaEmbeddings(model=\"nomic-embed-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c2f9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다국어 벡터 저장소 구축\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "db_openai = Chroma.from_documents(\n",
    "    documents=korean_docs+english_docs, \n",
    "    embedding=embeddings_openai,\n",
    "    collection_name=\"db_openai\",\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    )\n",
    "\n",
    "db_huggingface = Chroma.from_documents(\n",
    "    documents=korean_docs+english_docs, \n",
    "    embedding=embeddings_huggingface,\n",
    "    collection_name=\"db_huggingface\",\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    )\n",
    "\n",
    "db_ollama = Chroma.from_documents(\n",
    "    documents=korean_docs+english_docs, \n",
    "    embedding=embeddings_ollama,\n",
    "    collection_name=\"db_ollama\",\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    )\n",
    "\n",
    "# 벡터 저장소에 저장된 문서 수\n",
    "print(f\"OpenAI: {db_openai._collection.count()}\")\n",
    "print(f\"Hugging Face: {db_huggingface._collection.count()}\")\n",
    "print(f\"Ollama: {db_ollama._collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e791c6a",
   "metadata": {},
   "source": [
    "`(3) RAG 성능 비교 `  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fa08dbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG 체인 생성\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context.\n",
    "Do not use any external information or knowledge. \n",
    "If the answer is not in the context, answer \"잘 모르겠습니다.\".\n",
    "\n",
    "[Context]\n",
    "{context}\n",
    "\n",
    "[Question] \n",
    "{question}\n",
    "\n",
    "[Answer]\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "# 문서 포맷터 함수\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "\n",
    "# LLM 모델 생성\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# 체인 생성\n",
    "def create_rag_chain(vectorstore):\n",
    "\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={'k': 2})\n",
    "\n",
    "    return (\n",
    "        {\"context\": retriever | format_docs , \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "50d52cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체인 생성\n",
    "rag_chain_openai = create_rag_chain(db_openai)\n",
    "rag_chain_huggingface = create_rag_chain(db_huggingface)\n",
    "rag_chain_ollama = create_rag_chain(db_ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bc778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 쿼리에 대한 성능 평가\n",
    "query_ko = \"테슬라 창업자는 누구인가요?\"\n",
    "\n",
    "# OpenAI\n",
    "output_openai = rag_chain_openai.invoke(query_ko)\n",
    "print(\"OpenAI:\", output_openai)\n",
    "\n",
    "# Hugging Face\n",
    "output_huggingface = rag_chain_huggingface.invoke(query_ko)\n",
    "print(\"Hugging Face:\", output_huggingface)\n",
    "\n",
    "# Ollama\n",
    "output_ollama = rag_chain_ollama.invoke(query_ko)\n",
    "print(\"Ollama:\", output_ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc23ea46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영어 쿼리에 대한 성능 평가\n",
    "query_en = \"Who is the founder of Tesla?\"\n",
    "\n",
    "# OpenAI\n",
    "output_openai = rag_chain_openai.invoke(query_en)\n",
    "print(\"OpenAI:\", output_openai)\n",
    "\n",
    "# Hugging Face\n",
    "output_huggingface = rag_chain_huggingface.invoke(query_en)\n",
    "print(\"Hugging Face:\", output_huggingface)\n",
    "\n",
    "# Ollama\n",
    "output_ollama = rag_chain_ollama.invoke(query_en)\n",
    "print(\"Ollama:\", output_ollama)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70818e6",
   "metadata": {},
   "source": [
    "### 5-2 언어 감지 및 자동번역 통합 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19b80a6",
   "metadata": {},
   "source": [
    "`(1) 한국어 문서 벡터저장소 로드 `  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012a1ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 문서로 저장되어 있는 벡터 저장소 로드\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\", \n",
    ")\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=\"chroma_test\",\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    )\n",
    "\n",
    "print(f\"벡터 저장소에 저장된 문서 수: {vectorstore._collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21796bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepl\n",
    "from langdetect import detect\n",
    "\n",
    "translator = deepl.Translator(os.getenv('DEEPL_API_KEY'))\n",
    "\n",
    "def detect_and_translate(text, target_lang='KO'):\n",
    "    detected_lang = detect(text)\n",
    "    if detected_lang.upper() != target_lang:\n",
    "        result = translator.translate_text(text, target_lang=target_lang)\n",
    "        return str(result), detected_lang\n",
    "    return text, detected_lang\n",
    "\n",
    "# 문서 번역 테스트\n",
    "text = \"Hello. How are you?\"\n",
    "translated_text, detected_lang = detect_and_translate(text, target_lang='KO')\n",
    "print(f\"Detected language: {detected_lang}\")\n",
    "print(f\"Translated text: {translated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44368c9",
   "metadata": {},
   "source": [
    "`(2) RAG 체인 성능 평가 `  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bd44fdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={'k': 2})\n",
    "\n",
    "lang_rag_chain = (\n",
    "        {\"context\": retriever | format_docs , \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "# 체인 실행 함수\n",
    "def run_lang_rag_chain(query):\n",
    "    original_lang = detect(query)\n",
    "    \n",
    "    if original_lang.upper() != 'KO':\n",
    "        translated_query, _ = detect_and_translate(query, target_lang='KO')\n",
    "    else:\n",
    "        translated_query = query\n",
    "    \n",
    "    output = lang_rag_chain.invoke(translated_query)\n",
    "    \n",
    "    if original_lang.upper() != 'KO':\n",
    "        target_lang = 'EN-US' if original_lang.upper() == 'EN' else original_lang\n",
    "        output, _ = detect_and_translate(output, target_lang=target_lang)\n",
    "    \n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dea660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 쿼리에 대한 성능 평가\n",
    "query_ko = \"테슬라 창업자는 누구인가요?\"\n",
    "\n",
    "output = run_lang_rag_chain(query_ko)\n",
    "print(output)\n",
    "\n",
    "# 영어 쿼리에 대한 성능 평가\n",
    "query_en = \"Who is the founder of Tesla?\"\n",
    "\n",
    "output = run_lang_rag_chain(query_en)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031d5de6",
   "metadata": {},
   "source": [
    "### 5-3 언어 감지 및 벡터저장소 라우팅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea5168a",
   "metadata": {},
   "source": [
    "`(1) 언어별 벡터저장소 생성 `  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdd2bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 문서, 영어 문서를 각각 다른 벡터 저장소에 저장\n",
    "db_korean = Chroma.from_documents(\n",
    "    documents=korean_docs, \n",
    "    embedding=embeddings_huggingface,    # huggingface 임베딩 사용\n",
    "    collection_name=\"db_korean\",\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    )\n",
    "\n",
    "db_english = Chroma.from_documents(\n",
    "    documents=english_docs, \n",
    "    embedding=embeddings_ollama,        # ollama 임베딩 사용\n",
    "    collection_name=\"db_english\",\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    )\n",
    "\n",
    "# 벡터 저장소에 저장된 문서 수\n",
    "print(f\"한국어: {db_korean._collection.count()}\")\n",
    "print(f\"영어: {db_english._collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66434a3c",
   "metadata": {},
   "source": [
    "`(2) RAG 체인 성능 평가 `  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002d911c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체인 실행 함수\n",
    "from langdetect import detect\n",
    "\n",
    "rag_chain_korean = create_rag_chain(db_korean)\n",
    "rag_chain_english = create_rag_chain(db_english)\n",
    "\n",
    "\n",
    "def run_route_rag_chain(query):\n",
    "    original_lang = detect(query)\n",
    "    \n",
    "    if original_lang.upper() == 'KO':\n",
    "        return rag_chain_korean.invoke(query)\n",
    "        \n",
    "    elif original_lang.upper() == 'EN' :\n",
    "        return rag_chain_english.invoke(query)\n",
    "    \n",
    "    else:\n",
    "        return \"잘 모르겠습니다.\"\n",
    "    \n",
    "# 한국어 쿼리에 대한 성능 평가\n",
    "query_ko = \"테슬라 창업자는 누구인가요?\"\n",
    "\n",
    "output = run_route_rag_chain(query_ko)\n",
    "print(output)\n",
    "\n",
    "# 영어 쿼리에 대한 성능 평가\n",
    "query_en = \"Who is the founder of Tesla?\"\n",
    "\n",
    "output = run_route_rag_chain(query_en)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceb9f42",
   "metadata": {},
   "source": [
    "## 6. Gradio 챗봇"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57035b9",
   "metadata": {},
   "source": [
    "`(1) invoke 실행` \n",
    "\n",
    "- chat history 기능을 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b207180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# 답변 생성 모델을 별도로 사용\n",
    "answer_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "\n",
    "def answer_invoke(message, history):\n",
    "    history_langchain_format = []\n",
    "    for human, ai in history:\n",
    "        history_langchain_format.append(HumanMessage(content=human))\n",
    "        history_langchain_format.append(AIMessage(content=ai))\n",
    "    history_langchain_format.append(HumanMessage(content=message))\n",
    "\n",
    "    # 현재 메시지에 대해 RAG 체인 실행\n",
    "    rag_response = run_route_rag_chain(message)\n",
    "\n",
    "    # 답변 생성 모델에게 현재 메시지에 대한 답변 요청\n",
    "    final_answer = answer_llm.invoke(\n",
    "        history_langchain_format[:-1] + [AIMessage(content=rag_response)] + [HumanMessage(content=message)]\n",
    "    )\n",
    "    \n",
    "    return final_answer.content\n",
    "\n",
    "\n",
    "# Graiio 인터페이스 생성 \n",
    "demo = gr.ChatInterface(fn=answer_invoke, title=\"QA Bot\")\n",
    "\n",
    "# Graiio 실행  \n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f7a6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graiio 종료\n",
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b16b8d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
