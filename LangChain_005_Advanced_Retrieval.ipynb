{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a898547",
   "metadata": {},
   "source": [
    "## 1. 환경 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) LangSmith 설정 확인`\n",
    "- .env 파일에 아래 내용을 반영\n",
    "    - LANGCHAIN_TRACING_V2=true  \n",
    "    - LANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"  \n",
    "    - LANGCHAIN_API_KEY=\"인증키를 입력하세요\"  \n",
    "    - LANGCHAIN_PROJECT=\"프로젝트명\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0800c924",
   "metadata": {},
   "source": [
    "`(2) 기본 라이브러리`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bca641c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "from pprint import pprint\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839f4817",
   "metadata": {},
   "source": [
    "`(3) Env 환경변수`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f792283f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Langsmith tracing 여부를 확인 (true: langsmith 추적 활성화, false: langsmith 추적 비활성화)\n",
    "print(\"langsmith 추적 여부: \", os.getenv('LANGCHAIN_TRACING_V2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbc2ede",
   "metadata": {},
   "source": [
    "## 2.  벡터저장소 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a318126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터스토어 로드\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "chroma_db = Chroma(\n",
    "    embedding_function=embeddings_model,\n",
    "    collection_name=\"hf_bge_m3\",\n",
    "    persist_directory=\"./chroma_db\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f66661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 retriever 초기화\n",
    "chroma_k_retriever = chroma_db.as_retriever(\n",
    "    search_kwargs={\"k\": 2}\n",
    ")\n",
    "\n",
    "query = \"리비안의 성장 동력은 무엇인가요?\"\n",
    "retrieved_docs = chroma_k_retriever.invoke(query)\n",
    "\n",
    "print(f\"쿼리: {query}\")\n",
    "print(\"검색 결과:\")\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"- {doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"-\"*100)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449e370e",
   "metadata": {},
   "source": [
    "## 3. 고급 검색기법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916e40dc",
   "metadata": {},
   "source": [
    "### 3-1. 쿼리(Query) 확장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ed15b1",
   "metadata": {},
   "source": [
    "`(1) Multi Query`\n",
    "1. Retriever에 쿼리를 생성할 LLM을 지정\n",
    "1. LLM이 다양한 관점에서 여러 개의 쿼리를 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5b0bb6",
   "metadata": {},
   "source": [
    "- MultiQueryRetriever 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75e5683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 멀티 쿼리 생성\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model='gpt-4o-mini',\n",
    "    temperature=0.7,\n",
    "    max_tokens=100,\n",
    ")\n",
    "\n",
    "\n",
    "# 기본 retriever를 이용한 멀티 쿼리 생성 \n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=chroma_k_retriever, llm=llm\n",
    ")\n",
    "\n",
    "query = \"리비안의 성장 동력은 무엇인가요?\"\n",
    "retrieved_docs = multi_query_retriever.invoke(query)\n",
    "\n",
    "print(f\"쿼리: {query}\")\n",
    "print(\"검색 결과:\")\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"- {doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"-\"*100)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9bbe07",
   "metadata": {},
   "source": [
    "- Custom Prompt 활용  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609531ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# 모델 초기화\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# 출력 파서: LLM 결과를 질문 리스트로 변환\n",
    "class LineListOutputParser(BaseOutputParser[List[str]]):\n",
    "    \"\"\"Output parser for a list of lines.\"\"\"\n",
    "\n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        \"\"\"Split the text into lines and remove empty lines.\"\"\"\n",
    "        return [line.strip() for line in text.strip().split(\"\\n\") if line.strip()]\n",
    "    \n",
    "\n",
    "# 쿼리 생성 프롬프트\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"Generate three different versions of the given user question to retrieve relevant documents from a vector database. The goal is to reframe the question from various perspectives to overcome limitations of distance-based similarity search.\n",
    "\n",
    "    The generated questions should have the following characteristics:\n",
    "    1. Maintain the core intent of the original question but use different expressions or viewpoints.\n",
    "    2. Include synonyms or related concepts where possible.\n",
    "    3. Slightly broaden or narrow the scope of the question to potentially include diverse relevant information.\n",
    "\n",
    "    Write each question on a new line and include only the questions.\n",
    "\n",
    "    [Original question]\n",
    "    {question}\n",
    "    \n",
    "    [Alternative questions]\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "\n",
    "# 멀티쿼리 체인 구성\n",
    "multiquery_chain = QUERY_PROMPT | llm | LineListOutputParser()\n",
    "\n",
    "# 테스트 쿼리 실행\n",
    "query = \"리비안의 성장 동력은 무엇인가요?\"\n",
    "result = multiquery_chain.invoke({\"question\": query})\n",
    "\n",
    "print(\"생성된 대안 질문들:\")\n",
    "for i, q in enumerate(result, 1):\n",
    "    print(f\"{i}. {q}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48f3055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다중 쿼리 검색기 생성\n",
    "multi_query_custom_retriever = MultiQueryRetriever(\n",
    "    retriever=chroma_k_retriever, # 기본 retriever\n",
    "    llm_chain=multiquery_chain,   # 멀티쿼리 체인\n",
    "    parser_key=\"lines\"            # \"lines\": 출력 파서의 키\n",
    ")  \n",
    "\n",
    "retrieved_docs = multi_query_custom_retriever.invoke(query)\n",
    "\n",
    "print(f\"쿼리: {query}\")\n",
    "print(\"검색 결과:\")\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"- {doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"-\"*100)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7872201",
   "metadata": {},
   "source": [
    "`(2) Decomposition`\n",
    "  \n",
    "1. 입력된 질문을 여러 개의 하위 질문 또는 하위 문제로 분해 (LEAST-TO-MOST PROMPTING)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3b1871",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to decompose the given input question into multiple sub-questions. \n",
    "    The goal is to break down the input into a set of sub-problems/sub-questions that can be answered independently.\n",
    "\n",
    "    Follow these guidelines to generate the sub-questions:\n",
    "    1. Cover various aspects related to the core topic of the original question.\n",
    "    2. Each sub-question should be specific, clear, and answerable independently.\n",
    "    3. Ensure that the sub-questions collectively address all important aspects of the original question.\n",
    "    4. Consider temporal aspects (past, present, future) where applicable.\n",
    "    5. Formulate the questions in a direct and concise manner.\n",
    "\n",
    "    [Input question] \n",
    "    {question}\n",
    "\n",
    "    [Sub-questions (5)]\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "# 쿼리 생성 체인\n",
    "decomposition_chain = QUERY_PROMPT | llm | LineListOutputParser()\n",
    "\n",
    "# 테스트 쿼리 실행\n",
    "query = \"리비안의 성장 동력은 무엇인가요?\"\n",
    "result = decomposition_chain.invoke({\"question\": query})\n",
    "\n",
    "print(\"생성된 서브 질문들:\")\n",
    "for i, q in enumerate(result, 1):\n",
    "    print(f\"{i}. {q}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82567b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다중 쿼리 검색기 생성\n",
    "multi_query_decompostion_retriever = MultiQueryRetriever(\n",
    "    retriever=chroma_k_retriever,    # 기본 retriever\n",
    "    llm_chain=decomposition_chain,   # 서브 질문 생성 체인\n",
    "    parser_key=\"lines\"               # \"lines\": 출력 파서의 키\n",
    ")  \n",
    "\n",
    "retrieved_docs = multi_query_decompostion_retriever.invoke(query)\n",
    "\n",
    "print(f\"쿼리: {query}\")\n",
    "print(\"검색 결과:\")\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"- {doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"-\"*100)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(3) 검색 성능 평가`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607fb68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터셋 로드\n",
    "\n",
    "df_qa_test = pd.read_excel(\"./data/qa_test_revised.xlsx\")\n",
    "df_qa_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97856331",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_k_retriever.search_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f835f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가지표 계산\n",
    "from krag.utils import evaluate_retrieval_at_K\n",
    "\n",
    "retrievers = {\n",
    "    'top_k': chroma_k_retriever,\n",
    "    'multy_query': multi_query_retriever,\n",
    "    'multy_query_custom': multi_query_custom_retriever,\n",
    "    'multi_query_decompostion': multi_query_decompostion_retriever,\n",
    "}\n",
    "\n",
    "print(\"k:\", chroma_k_retriever.search_kwargs)\n",
    "\n",
    "df_evaluation, df_evaluation_data = evaluate_retrieval_at_K(\n",
    "    df_qa_test, \n",
    "    k=chroma_k_retriever.search_kwargs['k'],  \n",
    "    retrievers=retrievers, \n",
    "    ensemble=False, \n",
    "    rouge_method='rouge2', \n",
    "    threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ad4043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A/B 테스트 결과 출력 \n",
    "df_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecfc62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a2438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A/B 테스트 결과 시각화\n",
    "from krag.utils import visualize_retrieval_at_K\n",
    "\n",
    "visualize_retrieval_at_K(df_evaluation, k=chroma_k_retriever.search_kwargs['k'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. Re-rank\n",
    "- 재순위화는 검색 결과의 품질을 향상시키는 중요한 기법\n",
    "- 사용자의 쿼리와 관련성이 큰 문서들을 추출하여 상위에 위치시키는 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0daa21",
   "metadata": {},
   "source": [
    "`(1) Cross Encoder Reranker`\n",
    "\n",
    "1. Hugging Face의 Cross-Encoder 모델을 사용하여 검색 결과를 재정렬\n",
    "2. 크로스 인코더 아키텍처에서는 모델의 입력이 항상 데이터 쌍(예: 두 개의 문장 또는 문서)으로 구성되며, 이들은 인코더에 의해 공동으로 처리됨\n",
    "3. 크로스 인코더는 검색 쿼리와 검색된 문서 간의 유사성을 계산  \n",
    "https://www.sbert.net/examples/applications/cross-encoder/README.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f7adf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_k_retriever.search_kwargs[\"k\"] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd69791",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_k_retriever.search_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512fcb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "\n",
    "model = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-v2-m3\")\n",
    "re_ranker = CrossEncoderReranker(model=model, top_n=3)\n",
    "\n",
    "cross_encoder_reranker_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=re_ranker, \n",
    "    base_retriever=chroma_k_retriever,\n",
    ")\n",
    "\n",
    "question = \"테슬라 회장은 누구인가요?\"\n",
    "\n",
    "retrieved_docs = cross_encoder_reranker_retriever.invoke(question)\n",
    "\n",
    "print(f\"쿼리: {question}\")\n",
    "print(\"검색 결과:\")\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"- {doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"-\"*100)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e81e74",
   "metadata": {},
   "source": [
    "`(2) LLM Reranker`  \n",
    "\n",
    "- LLM을 사용하여 초기 검색된 문서 중에서 쿼리와 관련된 정도에 따라 순서를 조정\n",
    "- 예시: LLMListwiseRerank 등 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c816fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMListwiseRerank\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "re_ranker = LLMListwiseRerank.from_llm(llm, top_n=3)\n",
    "llm_reranker_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=re_ranker, \n",
    "    base_retriever=chroma_k_retriever,\n",
    ")\n",
    "\n",
    "question = \"테슬라 회장은 누구인가요?\"\n",
    "\n",
    "retrieved_docs = llm_reranker_retriever.invoke(question)\n",
    "\n",
    "print(f\"쿼리: {question}\")\n",
    "print(\"검색 결과:\")\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"- {doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"-\"*100)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6e689b",
   "metadata": {},
   "source": [
    "### 3-3. Contextual compression\n",
    "\n",
    "- 맥락적 압축은 검색된 문서를 그대로 반환하는 대신, 주어진 쿼리의 맥락을 사용하여 압축하는 기법\n",
    "- 쿼리와 관련된 정보만 반환되도록 하여 LLM 호출 비용을 줄이고 응답의 품질을 향상\n",
    "\n",
    "- 구성:\n",
    "    1. 기본 검색기(base retriever) \n",
    "    2. 문서 압축기(Document Compressor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e82658",
   "metadata": {},
   "source": [
    "`(1) LLMChainFilter`  \n",
    "\n",
    "- LLM을 사용하여 초기 검색된 문서 중 어떤 것을 필터링하고 어떤 것을 반환할지 결정\n",
    "- 문서 내용을 압축하거나 변경하지 않음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702729db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainFilter\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "context_filter = LLMChainFilter.from_llm(llm)\n",
    "\n",
    "llm_filter_compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=context_filter,             # LLM 기반 압축기\n",
    "    base_retriever=chroma_db.as_retriever(),          # 기본 검색기 \n",
    ")\n",
    "\n",
    "\n",
    "question = \"테슬라 회장은 누구인가요?\"\n",
    "\n",
    "compressed_docs = llm_filter_compression_retriever.invoke(question)\n",
    "\n",
    "print(f\"쿼리: {question}\")\n",
    "print(\"검색 결과:\")\n",
    "for doc in compressed_docs:\n",
    "    print(f\"- {doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"-\"*100)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1d3810",
   "metadata": {},
   "source": [
    "`(2) LLMChainExtractor`  \n",
    "\n",
    "- LLM을 사용하여 초기 반환된 문서를 순회하며 쿼리와 관련된 내용만 추출 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26aefec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "llm_extractor_compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,                                    # LLM 기반 압축기\n",
    "    base_retriever=cross_encoder_reranker_retriever,               # 기본 검색기 (Re-rank)\n",
    ")\n",
    "\n",
    "\n",
    "question = \"테슬라 회장은 누구인가요?\"\n",
    "\n",
    "compressed_docs = llm_extractor_compression_retriever.invoke(question)\n",
    "\n",
    "print(f\"쿼리: {question}\")\n",
    "print(\"검색 결과:\")\n",
    "for doc in compressed_docs:\n",
    "    print(f\"- {doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"-\"*100)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465dea88",
   "metadata": {},
   "source": [
    "`(3) EmbeddingsFilter`  \n",
    "\n",
    "- 문서와 쿼리를 임베딩하고 쿼리와 충분히 유사한 임베딩을 가진 문서만 반환\n",
    "- LLM을 사용하지 않는 방식 (LLM 호출보다 저렴하고 빠른 옵션)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96ee6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "\n",
    "embeddings_filter = EmbeddingsFilter(embeddings=embeddings_model, similarity_threshold=0.5)\n",
    "\n",
    "embed_filter_compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=embeddings_filter,                             # 임베딩 기반 압축기\n",
    "    base_retriever=cross_encoder_reranker_retriever,               # 기본 검색기 (Re-rank)\n",
    ")\n",
    "\n",
    "\n",
    "question = \"테슬라 회장은 누구인가요?\"\n",
    "\n",
    "compressed_docs = embed_filter_compression_retriever.invoke(question)\n",
    "\n",
    "print(f\"쿼리: {question}\")\n",
    "print(\"검색 결과:\")\n",
    "for doc in compressed_docs:\n",
    "    print(f\"- {doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"-\"*100)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38a156b",
   "metadata": {},
   "source": [
    "`(4) DocumentCompressorPipeline`  \n",
    "\n",
    "- 여러 압축기를 순차적으로 결합하는 방식\n",
    "- BaseDocumentTransformers를 추가하여, 문서를 더 작은 조각으로 나누거나 중복 문서를 제거하는 등의 작업도 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6303931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain_community.document_transformers import EmbeddingsRedundantFilter\n",
    "\n",
    "\n",
    "# 중복 문서 제거\n",
    "redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings_model)\n",
    "\n",
    "# 쿼리와 관련성이 높은 문서만 필터링\n",
    "relevant_filter = EmbeddingsFilter(embeddings=embeddings_model, similarity_threshold=0.5)\n",
    "\n",
    "# Re-ranking\n",
    "re_ranker = LLMListwiseRerank.from_llm(llm, top_n=2)\n",
    "\n",
    "pipeline_compressor = DocumentCompressorPipeline(\n",
    "    transformers=[redundant_filter, relevant_filter, re_ranker]\n",
    ")\n",
    "\n",
    "pipeline_compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=pipeline_compressor, \n",
    "    base_retriever=chroma_db.as_retriever()\n",
    ")\n",
    "\n",
    "question = \"테슬라 회장은 누구인가요?\"\n",
    "\n",
    "compressed_docs = pipeline_compression_retriever.invoke(question)\n",
    "\n",
    "print(f\"쿼리: {question}\")\n",
    "print(\"검색 결과:\")\n",
    "for doc in compressed_docs:\n",
    "    print(f\"- {doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"-\"*100)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3398c9f9",
   "metadata": {},
   "source": [
    "## 4. 답변 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3043ca3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context 없이 사전학습된 상태에서 답변 생성\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "llm.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41b65e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 쿼리에 대한 검색 결과를 한꺼번에 Context로 전달해서 답변을 생성\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the following question based on this context. If the context is not relevant to the question, just answer with '답변에 필요한 근거를 찾지 못했습니다.'\n",
    "\n",
    "[Context]\n",
    "{context}\n",
    "\n",
    "[Question]\n",
    "{question}\n",
    "\n",
    "[Answer]\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([f\"{doc.page_content}\" for doc in docs])\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": pipeline_compression_retriever | format_docs, \"question\": RunnablePassthrough()} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"테슬라 회장은 누구인가요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b76daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(\"리비안은 2011년에 어떤 차를 공개했나요?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
