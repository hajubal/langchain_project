{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4019d1fe",
   "metadata": {},
   "source": [
    "## 1. 환경 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608326bb",
   "metadata": {},
   "source": [
    "`(1) LangSmith 설정 확인`\n",
    "- .env 파일에 아래 내용을 반영\n",
    "    - LANGCHAIN_TRACING_V2=true  \n",
    "    - LANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"  \n",
    "    - LANGCHAIN_API_KEY=\"인증키를 입력하세요\"  \n",
    "    - LANGCHAIN_PROJECT=\"프로젝트명\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc3cefd",
   "metadata": {},
   "source": [
    "`(2) 기본 라이브러리`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ace4e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "from pprint import pprint\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34d6566",
   "metadata": {},
   "source": [
    "`(3) Env 환경변수`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7801daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Langsmith tracing 여부를 확인 (true: langsmith 추척 활성화, false: langsmith 추척 비활성화)\n",
    "print(\"langsmith 추척 여부: \", os.getenv('LANGCHAIN_TRACING_V2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc341c8c",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a116eb7",
   "metadata": {},
   "source": [
    "`(1) Raw Documents`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adad21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서를 로드\n",
    "from langchain_core.documents import Document\n",
    "import json\n",
    "\n",
    "final_docs = []\n",
    "\n",
    "with open('./data/final_docs_ver2.jsonl', 'rb') as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line)\n",
    "        doc = Document(page_content=item['page_content'], metadata=item['metadata'])\n",
    "        final_docs.append(doc)\n",
    "\n",
    "print(len(final_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dc73b2",
   "metadata": {},
   "source": [
    "`(2) Test Data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b162940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 데이터셋에 대한 QA 생성 결과를 리뷰한 후 다시 로드\n",
    "import pandas as pd\n",
    "\n",
    "df_qa_test = pd.read_excel(\"./data/qa_test_revised.xlsx\")\n",
    "\n",
    "df_qa_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681d2368",
   "metadata": {},
   "source": [
    "`(3) 검색도구 정의`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d9a519",
   "metadata": {},
   "source": [
    "-  BM25 검색기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9739e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25 검색기를 사용하기 위한 준비\n",
    "from krag.tokenizers import KiwiTokenizer\n",
    "from krag.retrievers import KiWiBM25RetrieverWithScore\n",
    "\n",
    "kiwi_tokenizer = KiwiTokenizer(model_type='knlm', typos='basic')\n",
    "\n",
    "bm25_db = KiWiBM25RetrieverWithScore(\n",
    "        documents=final_docs, \n",
    "        kiwi_tokenizer=kiwi_tokenizer, \n",
    "        k=2, \n",
    "        threshold=0.0,\n",
    "    )\n",
    "\n",
    "# BM25 검색기를 사용하여 문서 검색\n",
    "query = \"테슬라의 회장은 누구인가요?\"\n",
    "retrieved_docs = bm25_db.invoke(query, 2)\n",
    "\n",
    "# 검색 결과 출력 \n",
    "print(f\"쿼리: {query}\")\n",
    "print(\"검색 결과:\")\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"- {doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"-\"*100)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48202b63",
   "metadata": {},
   "source": [
    "- Vector Store 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb550c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터스토어 로드\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "chroma_db = Chroma(\n",
    "    embedding_function=embeddings_model,\n",
    "    collection_name=\"hf_bge_m3\",\n",
    "    persist_directory=\"./chroma_db\",\n",
    ")\n",
    "\n",
    "chroma_k = chroma_db.as_retriever(\n",
    "    search_kwargs={'k': 2},\n",
    ")\n",
    "\n",
    "# 벡터스토어를 사용하여 문서 검색\n",
    "query = \"테슬라의 회장은 누구인가요?\"\n",
    "\n",
    "retrieved_docs = chroma_k.invoke(query)\n",
    "\n",
    "# 검색 결과 출력\n",
    "print(f\"쿼리: {query}\")\n",
    "print(\"검색 결과:\")\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"- {doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"-\"*100)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec83a642",
   "metadata": {},
   "source": [
    "- Emsemble Hybrid Search 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17908b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "\n",
    "# 검색기 초기화 \n",
    "def create_hybrid_retriever(bm25_db, vector_db, k: int = 4):\n",
    "\n",
    "    bm25_db.k = k\n",
    "    chroma_k = vector_db.as_retriever(search_kwargs={'k': k})\n",
    "\n",
    "    retriever = EnsembleRetriever(\n",
    "        retrievers=[bm25_db, chroma_k],\n",
    "        weights=[0.5, 0.5],\n",
    "    )\n",
    "\n",
    "    return retriever\n",
    "\n",
    "hybrid_retriever = create_hybrid_retriever(bm25_db, chroma_db, k=4)\n",
    "\n",
    "query = \"테슬라의 회장은 누구인가요?\"\n",
    "retrieved_docs = hybrid_retriever.invoke(query)\n",
    "\n",
    "# 검색 결과 출력\n",
    "print(f\"쿼리: {query}\")\n",
    "print(\"검색 결과:\")\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"- {doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"-\"*100)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de555f3",
   "metadata": {},
   "source": [
    "- Cross-Encoder 알고리즘에 기반하여 재정렬 (Re-rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207ad64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "\n",
    "model = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-v2-m3\")\n",
    "re_ranker = CrossEncoderReranker(model=model, top_n=3)\n",
    "\n",
    "cross_encoder_reranker_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=re_ranker, \n",
    "    base_retriever=hybrid_retriever,\n",
    ")\n",
    "\n",
    "question = \"테슬라 회장은 누구인가요?\"\n",
    "\n",
    "retrieved_docs = cross_encoder_reranker_retriever.invoke(question)\n",
    "\n",
    "print(f\"쿼리: {question}\")\n",
    "print(\"검색 결과:\")\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"- {doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"-\"*100)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5781f4",
   "metadata": {},
   "source": [
    "## 3. LLM 유형 및 답변 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937f1066",
   "metadata": {},
   "source": [
    "### 3-1 RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b94247f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 쿼리에 대한 검색 결과를 한꺼번에 Context로 전달해서 답변을 생성\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "def create_rag_chain(retriever, llm):\n",
    "\n",
    "    template = \"\"\"Answer the following question based on this context. If the context is not relevant to the question, just answer with '답변에 필요한 근거를 찾지 못했습니다.'\n",
    "\n",
    "    [Context]\n",
    "    {context}\n",
    "\n",
    "    [Question]\n",
    "    {question}\n",
    "\n",
    "    [Answer]\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join([f\"{doc.page_content}\" for doc in docs])\n",
    "\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()} \n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e7552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG 체인 생성 및 테스트\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "openai_rag_chain = create_rag_chain(cross_encoder_reranker_retriever, llm)\n",
    "\n",
    "question = \"테슬라의 회장은 누구인가요?\"\n",
    "\n",
    "answer = openai_rag_chain.invoke(question)\n",
    "\n",
    "print(f\"쿼리: {question}\")\n",
    "print(\"답변:\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecc0afb",
   "metadata": {},
   "source": [
    "### 3-2 주요 모델 공급자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3539e9c1",
   "metadata": {},
   "source": [
    "`(1) Anthropic Claude API`\n",
    "\n",
    "https://www.anthropic.com/api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb5fbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatAnthropic - LLM 모델 \n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "# 모델 로드 \n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    temperature=0,\n",
    "    max_tokens=200, \n",
    ")\n",
    "\n",
    "# RAG 체인 생성 및 테스트\n",
    "anthropic_rag_chain = create_rag_chain(cross_encoder_reranker_retriever, llm)\n",
    "\n",
    "question = \"테슬라의 회장은 누구인가요?\"\n",
    "\n",
    "answer = anthropic_rag_chain.invoke(question)\n",
    "\n",
    "print(f\"쿼리: {question}\")\n",
    "print(\"답변:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7045556",
   "metadata": {},
   "source": [
    "`(2) Google Gemini API`\n",
    "\n",
    "https://ai.google.dev/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7564cf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGoogleGenerativeAI - LLM 모델 \n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# 모델 로드 \n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "\n",
    "# RAG 체인 생성 및 테스트\n",
    "google_genai_rag_chain = create_rag_chain(cross_encoder_reranker_retriever, llm)\n",
    "\n",
    "question = \"테슬라의 회장은 누구인가요?\"\n",
    "\n",
    "answer = google_genai_rag_chain.invoke(question)\n",
    "\n",
    "print(f\"쿼리: {question}\")\n",
    "print(\"답변:\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988de355",
   "metadata": {},
   "source": [
    "`(3) Ollama - 오픈소스 LLM`\n",
    "\n",
    "https://ollama.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d20b039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatOllama - LLM 모델 \n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# 모델 로드 \n",
    "llm = ChatOllama(\n",
    "    model = \"llama3.1\",\n",
    "    temperature = 0.8,\n",
    "    num_predict = 100,\n",
    ")\n",
    "\n",
    "# RAG 체인 생성 및 테스트\n",
    "ollama_rag_chain = create_rag_chain(cross_encoder_reranker_retriever, llm)\n",
    "\n",
    "question = \"테슬라의 회장은 누구인가요?\"\n",
    "\n",
    "answer = ollama_rag_chain.invoke(question)\n",
    "\n",
    "print(f\"쿼리: {question}\")\n",
    "print(\"답변:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e78dd2",
   "metadata": {},
   "source": [
    "`(4) Groq API - 오픈소스 LLM`\n",
    "\n",
    "https://groq.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a2e8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGroq - LLM 모델 \n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# 모델 로드 \n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-70b-versatile\",\n",
    "    temperature=0.0,\n",
    "    max_tokens=100,\n",
    ")\n",
    "\n",
    "# RAG 체인 생성 및 테스트\n",
    "groq_rag_chain = create_rag_chain(cross_encoder_reranker_retriever, llm)\n",
    "\n",
    "question = \"테슬라의 회장은 누구인가요?\"\n",
    "\n",
    "answer = groq_rag_chain.invoke(question)\n",
    "\n",
    "print(f\"쿼리: {question}\")\n",
    "print(\"답변:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04483861",
   "metadata": {},
   "source": [
    "## 4. RAG 답변 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfb75c5",
   "metadata": {},
   "source": [
    "### 4-1. Metric Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678f7f18",
   "metadata": {},
   "source": [
    "`(1) Embedding Distance`\n",
    "- 임베딩 거리를 사용하여 예측 문자열과 참조 레이블 문자열 간의 의미적 유사성을 측정\n",
    "- 계산된 거리 점수가 낮을수록 두 문자열의 의미가 더 유사함을 나타내며, 이 방법은 단순 문자열 비교보다 더 풍부한 의미적 평가가 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1ac065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain EmbeddingDistanceEvalChain 활용 \n",
    "\n",
    "from langchain.evaluation import load_evaluator, EvaluatorType, EmbeddingDistance\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Evaluator 초기화\n",
    "embedding_evaluator = load_evaluator(\n",
    "    evaluator=EvaluatorType.EMBEDDING_DISTANCE,      # 임베딩 거리를 기반으로 평가\n",
    "    distance_metric=EmbeddingDistance.COSINE,        # 코사인 유사도 사용\n",
    "    llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0), # OpenAI LLM 사용\n",
    "    )\n",
    "\n",
    "\n",
    "# 의미가 다른 문장 비교\n",
    "result1 = embedding_evaluator.evaluate_strings(prediction=\"나는 학교에 갈 것이다\", reference=\"나는 집에 있을 것이다\")\n",
    "print(\"의미가 다른 문장 비교 결과:\", result1)\n",
    "\n",
    "# 의미가 비슷한 문장 비교\n",
    "result2 = embedding_evaluator.evaluate_strings(prediction=\"나는 학교에 갈 것이다\", reference=\"나는 학교로 향할 것이다\")\n",
    "print(\"의미가 비슷한 문장 비교 결과:\", result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14549cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 데이터셋에 대한 평가\n",
    "df_qa_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb96601e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫번째 샘플에 대해 평가 - ground truth와 비교\n",
    "\n",
    "question = df_qa_test.iloc[0]['question']\n",
    "ground_truth = df_qa_test.iloc[0]['answer']\n",
    "\n",
    "print(\"Question:\", question)\n",
    "print(\"Ground Truth:\", ground_truth)\n",
    "\n",
    "# OpenAI LLM을 사용하여 예측 생성\n",
    "openai_prediction = openai_rag_chain.invoke(question)\n",
    "print(\"Prediction:\", openai_prediction)\n",
    "\n",
    "distance_score = embedding_evaluator.evaluate_strings(prediction=openai_prediction, reference=ground_truth)\n",
    "print(\"Distance Score:\", distance_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef27968e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 다른 모델들에 대해 평가\n",
    "# Anthropiic LLM을 사용하여 예측 생성\n",
    "anthropic_prediction = anthropic_rag_chain.invoke(question)\n",
    "print(\"Prediction:\", anthropic_prediction)\n",
    "\n",
    "distance_score = embedding_evaluator.evaluate_strings(prediction=anthropic_prediction, reference=ground_truth)\n",
    "print(\"Distance Score:\", distance_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e464eeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Generative AI LLM을 사용하여 예측 생성\n",
    "google_genai_prediction = google_genai_rag_chain.invoke(question)\n",
    "print(\"Prediction:\", google_genai_prediction)\n",
    "\n",
    "distance_score = embedding_evaluator.evaluate_strings(prediction=google_genai_prediction, reference=ground_truth)\n",
    "print(\"Distance Score:\", distance_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38d0726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama LLM을 사용하여 예측 생성\n",
    "ollama_prediction = ollama_rag_chain.invoke(question)\n",
    "print(\"Prediction:\", ollama_prediction)\n",
    "\n",
    "distance_score = embedding_evaluator.evaluate_strings(prediction=ollama_prediction, reference=ground_truth)\n",
    "print(\"Distance Score:\", distance_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b201a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groq LLM을 사용하여 예측 생성\n",
    "groq_prediction = groq_rag_chain.invoke(question)\n",
    "print(\"Prediction:\", groq_prediction)\n",
    "\n",
    "distance_score = embedding_evaluator.evaluate_strings(prediction=groq_prediction, reference=ground_truth)\n",
    "print(\"Distance Score:\", distance_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2dc8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 모델에 대해 평가 결과를 비교\n",
    "\n",
    "def embedding_evaluate_all_models(question, ground_truth):\n",
    "    results = {}\n",
    "    for model_name, rag_chain in {\n",
    "        \"OpenAI\": openai_rag_chain,\n",
    "        \"Anthropic\": anthropic_rag_chain,\n",
    "        \"Google\": google_genai_rag_chain,\n",
    "        \"Ollama\": ollama_rag_chain,\n",
    "        \"Groq\": groq_rag_chain,\n",
    "    }.items():\n",
    "        prediction = rag_chain.invoke(question)\n",
    "        distance_score = embedding_evaluator.evaluate_strings(prediction=prediction, reference=ground_truth)\n",
    "        results[model_name] = f\"{distance_score['score']:.3f}\"\n",
    "    return results\n",
    "\n",
    "question = df_qa_test.iloc[0]['question']\n",
    "ground_truth = df_qa_test.iloc[0]['answer']\n",
    "\n",
    "results = embedding_evaluate_all_models(question, ground_truth)\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa7b714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 데이터셋에 대해 A/B 테스트 - 여기서는 3개만 평가 (데이터프레임으로 정리)\n",
    "\n",
    "def embedding_evaluate_qa_dataset(df_qa_test):\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i in range(3):\n",
    "        question = df_qa_test.iloc[i]['question']\n",
    "        ground_truth = df_qa_test.iloc[i]['answer']\n",
    "        result = embedding_evaluate_all_models(question, ground_truth)\n",
    "        results.append(result)\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "df_result_embedding = embedding_evaluate_qa_dataset(df_qa_test.iloc[:3])\n",
    "df_result_embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1df79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Distance가 낮을 수록 좋은 결과\n",
    "df_result_embedding.astype(float).mean().sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f520bf",
   "metadata": {},
   "source": [
    "`(2) Cross-Encoder 활용`\n",
    "- 두 문장을 동시에 인코딩하여 직접적으로 유사성을 평가 (개별 문장을 따로 인코딩하는 일반적인 Bi-Encoder 임베딩 방식과 다름)\n",
    "- 크로스 인코더는 두 문장의 상호작용을 직접 모델링하므로, 일반적으로 더 정확한 유사성 점수를 제공\n",
    "- 이 방법은 의미적 유사성을 더 정확하게 측정할 수 있지만, 계산 비용이 더 높다는 점을 유의해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0d83027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "def calculate_cross_encoder_similarity(\n",
    "        query: str, \n",
    "        prediction: str, \n",
    "        model_name: str = \"BAAI/bge-reranker-v2-m3\",\n",
    "        ) -> float:\n",
    "    \"\"\"\n",
    "    주어진 query와 prediction 사이의 의미적 유사성을 계산합니다.\n",
    "\n",
    "    Args:\n",
    "    query (str): 기준이 되는 쿼리 문장\n",
    "    prediction (str): 유사성을 비교할 예측 문장\n",
    "    model_name (str): 사용할 크로스 인코더 모델 이름 (기본값: \"BAAI/bge-reranker-v2-m3\")\n",
    "\n",
    "    Returns:\n",
    "    float: 두 문장 간의 유사성 점수\n",
    "    \"\"\"\n",
    "    # 크로스 인코더 모델을 불러옵니다.\n",
    "    cross_encoder_model = CrossEncoder(model_name)\n",
    "\n",
    "    # 크로스 인코더를 사용하여 유사성 점수를 계산합니다.\n",
    "    sentence_pairs = [[query, prediction]]\n",
    "    similarity_scores = cross_encoder_model.predict(sentence_pairs)\n",
    "\n",
    "    return similarity_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be6ae16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫 번째 샘플에 대해 유사성 점수 계산 - 점수가 높을수록 더 유사함\n",
    "\n",
    "print(\"Question:\", question)\n",
    "print(\"Ground Truth:\", ground_truth)\n",
    "print(\"Prediction:\", openai_prediction)\n",
    "\n",
    "similarity = calculate_cross_encoder_similarity(ground_truth, openai_prediction)\n",
    "print(f\"유사성 점수: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3404f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 모델에 대해 평가 결과를 비교\n",
    "\n",
    "def cross_encoder_evaluate_all_models(question, ground_truth):\n",
    "    results = {}\n",
    "    for model_name, rag_chain in {\n",
    "        \"OpenAI\": openai_rag_chain,\n",
    "        \"Anthropic\": anthropic_rag_chain,\n",
    "        \"Google\": google_genai_rag_chain,\n",
    "        \"Ollama\": ollama_rag_chain,\n",
    "        \"Groq\": groq_rag_chain,\n",
    "    }.items():\n",
    "        prediction = rag_chain.invoke(question)\n",
    "        print(model_name)\n",
    "        print(prediction)\n",
    "        print()\n",
    "        similarity = calculate_cross_encoder_similarity(ground_truth, prediction)\n",
    "        results[model_name] = f\"{similarity:.3f}\"\n",
    "    return results\n",
    "\n",
    "\n",
    "# 전체 데이터셋에 대해 A/B 테스트 - 여기서는 3개만 평가 (데이터프레임으로 정리)\n",
    "\n",
    "def cross_encoder_evaluate_qa_dataset(df_qa_test):\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i in range(3):\n",
    "        question = df_qa_test.iloc[i]['question']\n",
    "        ground_truth = df_qa_test.iloc[i]['answer']\n",
    "        result = cross_encoder_evaluate_all_models(question, ground_truth)\n",
    "        results.append(result)\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "df_result_cross_encoder = cross_encoder_evaluate_qa_dataset(df_qa_test.iloc[:3])\n",
    "df_result_cross_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f292ddee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Encoder Similarity가 높을 수록 좋은 결과\n",
    "df_result_cross_encoder.astype(float).mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b24443",
   "metadata": {},
   "source": [
    "`(3) Rouge Metric`\n",
    "- ROUGE 메트릭은 두 문장의 유사도를 평가하는 데 널리 사용되는 방법으로, 특히 텍스트 요약과 기계 번역 분야에서 유용함\n",
    "- 단어 중첩을 기반으로 하여 계산이 빠르고 해석이 쉽다는 장점이 있지만, 깊은 의미적 유사성을 포착하는 데는 한계가 있음\n",
    "- 사용 목적과 컨텍스트에 따라 적절히 선택하거나 다른 메트릭과 조합하여 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "38708625",
   "metadata": {},
   "outputs": [],
   "source": [
    "from korouge_score import rouge_scorer\n",
    "from typing import List, Dict\n",
    "\n",
    "def calculate_rouge_similarity(\n",
    "        query: str, \n",
    "        prediction: str, \n",
    "        rouge_types: List[str] = ['rouge1', 'rouge2', 'rougeL'],\n",
    "        ) -> Dict[str, float]:\n",
    "    \n",
    "    \"\"\"\n",
    "    주어진 쿼리 문장과 예측 문장 사이의 선택된 ROUGE 점수를 계산합니다.\n",
    "\n",
    "    Args:\n",
    "    query (str): 기준이 되는 쿼리 문장\n",
    "    prediction (str): 유사성을 비교할 예측 문장\n",
    "    rouge_types (List[str]): 계산할 ROUGE 메트릭 리스트 (기본값: ['rouge1', 'rouge2', 'rougeL'])\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, float]: 선택된 ROUGE 메트릭의 F1 점수를 포함하는 딕셔너리\n",
    "    \"\"\"\n",
    "    # 입력된 ROUGE 유형의 유효성을 검사합니다.\n",
    "    valid_rouge_types = set(['rouge1', 'rouge2', 'rougeL'])\n",
    "    rouge_types = [rt for rt in rouge_types if rt in valid_rouge_types]\n",
    "    \n",
    "    if not rouge_types:\n",
    "        raise ValueError(\"유효한 ROUGE 유형이 제공되지 않았습니다.\")\n",
    "\n",
    "    # ROUGE scorer 객체를 초기화합니다.\n",
    "    scorer = rouge_scorer.RougeScorer(rouge_types, use_stemmer=True)\n",
    "\n",
    "    # ROUGE 점수를 계산합니다.\n",
    "    scores = scorer.score(query, prediction)\n",
    "\n",
    "    # 결과를 정리합니다.\n",
    "    result = {rouge_type: scores[rouge_type].fmeasure for rouge_type in rouge_types}\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30f525d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫 번째 샘플에 대해 유사성 점수 계산 - 점수가 높을수록 더 유사함\n",
    "\n",
    "print(\"Question:\", question)\n",
    "print(\"Ground Truth:\", ground_truth)\n",
    "print(\"Prediction:\", openai_prediction)\n",
    "\n",
    "rouge_scores = calculate_rouge_similarity(ground_truth, openai_prediction, ['rouge1'])\n",
    "print(f\"Rouge 점수: {rouge_scores['rouge1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b523331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 모델에 대해 평가 결과를 비교\n",
    "\n",
    "def rouge_evaluate_all_models(question, ground_truth):\n",
    "    results = {}\n",
    "    for model_name, rag_chain in {\n",
    "        \"OpenAI\": openai_rag_chain,\n",
    "        \"Anthropic\": anthropic_rag_chain,\n",
    "        \"Google\": google_genai_rag_chain,\n",
    "        \"Ollama\": ollama_rag_chain,\n",
    "        \"Groq\": groq_rag_chain,\n",
    "    }.items():\n",
    "        prediction = rag_chain.invoke(question)\n",
    "        rouge_scores = calculate_rouge_similarity(ground_truth, prediction, ['rouge2'])\n",
    "        results[model_name] = f\"{rouge_scores['rouge2']:.3f}\"\n",
    "    return results\n",
    "\n",
    "# 전체 데이터셋에 대해 A/B 테스트 - 여기서는 3개만 평가 (데이터프레임으로 정리)\n",
    "\n",
    "def rouge_evaluate_qa_dataset(df_qa_test):\n",
    "    \n",
    "        results = []\n",
    "    \n",
    "        for i in range(3):\n",
    "            question = df_qa_test.iloc[i]['question']\n",
    "            ground_truth = df_qa_test.iloc[i]['answer']\n",
    "            result = rouge_evaluate_all_models(question, ground_truth)\n",
    "            results.append(result)\n",
    "    \n",
    "        return pd.DataFrame(results)    \n",
    "\n",
    "df_result_rouge = rouge_evaluate_qa_dataset(df_qa_test.iloc[:3])\n",
    "df_result_rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd98ef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE 점수가 높을 수록 좋은 결과\n",
    "df_result_rouge.astype(float).mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b56bb2",
   "metadata": {},
   "source": [
    "### 4-2. LLM-as-judge\n",
    "- LLM은 인간과 유사한 판단을 제공할 수 있어, 단순한 단어 중첩 기반 메트릭보다 더 깊은 의미적 관련성을 포착할 수 있음\n",
    "- ROUGE와 같은 전통적인 메트릭보다 더 정교한 평가를 제공할 수 있지만, LLM의 판단에 의존하기 때문에 완전한 객관성을 보장하기는 어려움\n",
    "- 또한 API 사용에 따른 비용과 처리 시간이 필요하다는 점을 고려해야 함\n",
    "- 따라서 여러 평가 방법을 조합하여 사용 필요 (예를 들어, ROUGE 점수로 빠른 초기 스크리닝을 하고, 중요한 케이스에 대해서만 이 LLM 기반 평가를 수행하는 방식을 고려)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63659d6c",
   "metadata": {},
   "source": [
    "`(1) QA Evaluation - 사용자 질문에 대한 정확성, 관련성을 평가 (Y/N, 0/1)`\n",
    "\n",
    "1. \"qa\" 평가기: 사용자 질문에 대한 응답의 정확성을 직접적으로 평가\n",
    "\n",
    "2. \"context_qa\" 평가기: 더 넓은 맥락을 고려하여 응답의 정확성을 평가\n",
    "\n",
    "3. \"cot_qa\" 평가기: CoT(Chain of Thought) 추론을 통해 더 심층적인 평가를 수행\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde0a0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2번째 샘플에 대해 예측 수행\n",
    "question = df_qa_test.iloc[1]['question']\n",
    "context = df_qa_test.iloc[1]['context']\n",
    "ground_truth = df_qa_test.iloc[1]['answer']\n",
    "groq_prediction = groq_rag_chain.invoke(question)\n",
    "\n",
    "print(\"Question:\", question)\n",
    "print(\"Context:\", context)\n",
    "print(\"Ground Truth:\", ground_truth)\n",
    "print(\"Prediction:\", groq_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf9912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator, EvaluatorType\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# QA Evaluator 초기화\n",
    "qa_evaluator = load_evaluator(\n",
    "    evaluator=\"qa\",  \n",
    "    llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0), # OpenAI LLM 사용\n",
    "    )\n",
    "\n",
    "# 2번째 샘플에 대해 평가 수행\n",
    "qa_eval_result = qa_evaluator.evaluate_strings(\n",
    "    input=question,               # 평가에 고려할 내용: 질문\n",
    "    prediction=groq_prediction,   # 평가 대상: LLM 모델의 예측\n",
    "    reference=ground_truth,       # 평가 기준: 정답\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "qa_eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2665a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context QA Evaluator 초기화\n",
    "context_qa_evaluator = load_evaluator(\n",
    "    evaluator=\"context_qa\",  \n",
    "    llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0), # OpenAI LLM 사용\n",
    "    )\n",
    "\n",
    "# 2번째 샘플에 대해 평가 수행\n",
    "context_qa_eval_result = context_qa_evaluator.evaluate_strings(\n",
    "    input=question,               # 평가에 고려할 내용: 질문\n",
    "    prediction=groq_prediction,   # 평가 대상: LLM 모델의 예측\n",
    "    reference=context,            # 평가 기준: 문맥\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "context_qa_eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca74379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COT QA Evaluator 초기화\n",
    "cot_qa_evaluator = load_evaluator(\n",
    "    evaluator=\"cot_qa\",  \n",
    "    llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0), # OpenAI LLM 사용\n",
    "    )\n",
    "\n",
    "# 2번째 샘플에 대해 평가 수행\n",
    "cot_qa_eval_result = cot_qa_evaluator.evaluate_strings(\n",
    "    input=question,               # 평가에 고려할 내용: 질문\n",
    "    prediction=groq_prediction,   # 평가 대상: LLM 모델의 예측\n",
    "    reference=context,            # 평가 기준: 문맥\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "cot_qa_eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25d6732",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cot_qa_eval_result[\"reasoning\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9705e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COT QA Evaluator 초기화 (사용자 정의 프롬프트 사용)\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "\n",
    "CUSTOM_COT_QA_PROMPT = \"\"\"\n",
    "You are an expert evaluating the performance of a RAG (Retrieval-Augmented Generation) system. Your task is to assess the quality of the system's answer based on the given question, retrieved context, and the generated answer. Grade the answer as CORRECT, PARTIALLY CORRECT, or INCORRECT, and provide a detailed explanation. Please describe your evaluation process step by step to clearly show how you reached your conclusion.\n",
    "\n",
    "Use the following criteria for evaluation:\n",
    "1. Factual Accuracy: Does the answer align with the information in the retrieved context?\n",
    "2. Relevance: Does the answer appropriately address the question?\n",
    "3. Completeness: Does the answer cover all aspects of the question?\n",
    "4. Conciseness: Does the answer convey the key information without unnecessary details?\n",
    "\n",
    "Grading Criteria:\n",
    "- CORRECT: Meets all criteria with no errors\n",
    "- PARTIALLY CORRECT: Meets some criteria but has minor errors or omissions\n",
    "- INCORRECT: Contains major factual errors or is irrelevant to the question\n",
    "\n",
    "Evaluation Format:\n",
    "QUESTION: [The question content]\n",
    "CONTEXT: [The retrieved context]\n",
    "SYSTEM ANSWER: [The answer generated by the RAG system]\n",
    "EVALUATION:\n",
    "1. Factual Accuracy: [Assessment and explanation]\n",
    "2. Relevance: [Assessment and explanation]\n",
    "3. Completeness: [Assessment and explanation]\n",
    "4. Conciseness: [Assessment and explanation]\n",
    "Overall Assessment: [General evaluation summary]\n",
    "GRADE: [CORRECT / PARTIALLY CORRECT / INCORRECT]\n",
    "\n",
    "Now, please evaluate the RAG system's answer based on the provided information. Prefer to use in Korean language.\n",
    "\n",
    "QUESTION: {query}\n",
    "CONTEXT: {context}\n",
    "SYSTEM ANSWER: {result}\n",
    "EVALUATION:\n",
    "\"\"\"\n",
    "\n",
    "custom_qa_prompt = PromptTemplate.from_template(CUSTOM_COT_QA_PROMPT)\n",
    "\n",
    "custom_cot_qa_evaluator = load_evaluator(\n",
    "    evaluator=\"cot_qa\",  \n",
    "    llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0), # OpenAI LLM 사용\n",
    "    prompt=custom_qa_prompt                               # 사용자 지정 프롬프트 사용\n",
    "    )\n",
    "\n",
    "# 2번째 샘플에 대해 평가 수행\n",
    "custom_cot_qa_eval_result = custom_cot_qa_evaluator.evaluate_strings(\n",
    "    input=question,               # 평가에 고려할 내용: 질문\n",
    "    prediction=groq_prediction,   # 평가 대상: LLM 모델의 예측\n",
    "    reference=context,            # 평가 기준: 문맥\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "custom_cot_qa_eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe1f1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(custom_cot_qa_eval_result['reasoning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f789d082",
   "metadata": {},
   "source": [
    "`(2) Criteria Evaluation (No lables) - 참조 레이블(ground truth)이 없는 상황에서 모델 출력의 품질을 평가`\n",
    "\n",
    "1. \"criteria\" 평가기:\n",
    "   - 목적: 주어진 기준에 따라 예측이 기준을 만족하는지 평가\n",
    "   - 출력: 이진 점수 (예: Yes/No 또는 1/0)\n",
    "\n",
    "2. \"score_string\" 평가기:\n",
    "   - 목적: 주어진 기준에 따라 예측의 품질을 수치로 평가\n",
    "   - 출력: 수치 점수 (기본적으로 1-10 척도)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87c4b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "# 간결성 평가 - criteria 평가자 사용\n",
    "conciseness_evaluator = load_evaluator(\n",
    "    evaluator=\"criteria\", \n",
    "    criteria=\"conciseness\",\n",
    "    llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0),\n",
    "    )\n",
    "\n",
    "# 2번째 샘플에 대해 평가 수행\n",
    "conciseness_result = conciseness_evaluator.evaluate_strings(\n",
    "    input=question,                   # 질문 \n",
    "    prediction=groq_prediction,       # 평가 대상: LLM 모델의 예측\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "conciseness_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9bb4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conciseness_result['reasoning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dff0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# criteria 직접 지정\n",
    "criteria_evaluator = load_evaluator(\n",
    "    evaluator=\"criteria\", \n",
    "    criteria={\n",
    "        \"relevance\": \"Does the answer appropriately address the question?\",\n",
    "        \"conciseness\": \"Does the answer convey the key information without unnecessary details?\",\n",
    "        },\n",
    "    llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0),\n",
    "    )\n",
    "\n",
    "# 2번째 샘플에 대해 평가 수행\n",
    "criteria_result = criteria_evaluator.evaluate_strings(\n",
    "    input=question,              # 질문 \n",
    "    prediction=groq_prediction,      # 평가 대상: LLM 모델의 예측\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "criteria_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaac3ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(criteria_result['reasoning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41934c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_string 평가자 사용\n",
    "score_string_evaluator = load_evaluator(\n",
    "    evaluator=\"score_string\", \n",
    "    criteria={\n",
    "        \"relevance\": \"How relevant is the answer to the question on a scale of 1-10?\",\n",
    "    },\n",
    "    normalize_by=10,\n",
    "    llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0),\n",
    "    )\n",
    "\n",
    "# 2번째 샘플에 대해 평가 수행\n",
    "score_string_result = score_string_evaluator.evaluate_strings(\n",
    "    input=question,                     # 질문 \n",
    "    prediction=groq_prediction,         # 평가 대상: LLM 모델의 예측\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "score_string_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744e6f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score_string_result['reasoning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec4d4d6",
   "metadata": {},
   "source": [
    "`(3) Criteria Evaluation (With lables) - 참조 레이블(ground truth)이 주어진 상황에서 모델 출력의 품질을 평가`\n",
    "\n",
    "1. \"labeled_criteria\" 평가기:\n",
    "   - 목적: 참조 레이블을 고려하여 예측이 주어진 기준을 만족하는지 평가\n",
    "   - 출력: 이진 점수 (예: Yes/No 또는 1/0)\n",
    "\n",
    "2. \"labeled_score_string\" 평가기:\n",
    "   - 목적: 참조 레이블과 비교하여 예측의 품질을 수치로 평가\n",
    "   - 출력: 수치 점수 (기본적으로 1-10 척도)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76743974",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "# labeled_criteria 평가자 사용\n",
    "labeled_crieria_evaluator = load_evaluator(\n",
    "    evaluator=\"labeled_criteria\", \n",
    "    criteria=\"correctness\",\n",
    "    llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0),\n",
    "    )\n",
    "\n",
    "# 2번째 샘플에 대해 평가 수행\n",
    "labeled_crieria_eval_result = labeled_crieria_evaluator.evaluate_strings(\n",
    "    input=question,               # 평가에 고려할 내용: 질문\n",
    "    prediction=groq_prediction,   # 평가 대상: LLM 모델의 예측\n",
    "    reference=ground_truth,       # 평가 기준: 정답\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "labeled_crieria_eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a5e426",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labeled_crieria_eval_result['reasoning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddcd0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labeled_score_string 평가자 사용\n",
    "labeled_score_string_evaluator = load_evaluator(\n",
    "    evaluator=\"labeled_score_string\", \n",
    "    criteria=\"correctness\",\n",
    "    normalize_by=10,\n",
    "    llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0),\n",
    "    )\n",
    "\n",
    "# 2번째 샘플에 대해 평가 수행\n",
    "labeled_score_string_eval_result = labeled_score_string_evaluator.evaluate_strings(\n",
    "    input=question,               # 평가에 고려할 내용: 질문\n",
    "    prediction=groq_prediction,   # 평가 대상: LLM 모델의 예측\n",
    "    reference=ground_truth,       # 평가 기준: 정답\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "labeled_score_string_eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ab755a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labeled_score_string_eval_result['reasoning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d831d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트를 직접 작성하여 평가 수행 (한국어로 평가)\n",
    "\n",
    "from langchain.evaluation import load_evaluator, EvaluatorType\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "correctness_eval_template = \"\"\"Respond Y or N based on how accurate the given response is compared to the expected answer. Grade only based on the rubric and expected answer:\n",
    "\n",
    "Grading Rubric: {criteria}\n",
    "Question: {input}\n",
    "Expected Answer: {reference}\n",
    "\n",
    "DATA:\n",
    "---------\n",
    "Actual Response: {output}\n",
    "---------\n",
    "Write out your explanation for how accurate the response is compared to the expected answer, considering factual correctness and completeness. Then respond with Y or N on a new line. (in Korean)\"\"\"\n",
    "\n",
    "correctness_eval_prompt = PromptTemplate.from_template(correctness_eval_template)\n",
    "\n",
    "# Correctness: 생성된 답변이 ground truth와 비교하여 정확하고 사실적인지 평가\n",
    "correctness_evaluator = load_evaluator(\n",
    "    evaluator=\"labeled_criteria\", \n",
    "    criteria=\"correctness\",  \n",
    "    llm=ChatOpenAI(model=\"gpt-4o\", temperature=0.0), \n",
    "    prompt=correctness_eval_prompt # 사용자 지정 프롬프트 사용\n",
    "    )\n",
    "\n",
    "\n",
    "# 2번째 샘플에 대해 평가 수행\n",
    "correctness_eval_result = correctness_evaluator.evaluate_strings(\n",
    "    input=question,\n",
    "    prediction=groq_prediction,\n",
    "    reference=ground_truth,\n",
    ")\n",
    "\n",
    "print(f'Correctness Score: {correctness_eval_result[\"score\"]}')\n",
    "print(f'Correctness Reasoniong: {correctness_eval_result[\"reasoning\"]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286c9181",
   "metadata": {},
   "outputs": [],
   "source": [
    "correctness_eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5d6461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labled_score_string 평가\n",
    "correctness_score_template = \"\"\"\n",
    "[Instruction]\n",
    "Please act as an impartial judge and evaluate the correctness of the AI assistant's response compared to the ground truth. \n",
    "{criteria}\n",
    "\n",
    "[Ground Truth]\n",
    "{reference}\n",
    "\n",
    "Begin your evaluation by providing a short explanation. Be as objective as possible. Answer in Korean. After providing your explanation, you must rate the response on a scale of 1 to 5 by strictly following this format: \"[[rating]]\", for example: \"Rating: [[3]]\".\n",
    "\n",
    "[Question]\n",
    "{input}\n",
    "\n",
    "[The Start of Assistant's Answer]\n",
    "{prediction}\n",
    "[The End of Assistant's Answer]\n",
    "\"\"\"\n",
    "\n",
    "correctness_score_prompt = PromptTemplate.from_template(correctness_score_template)\n",
    "\n",
    "correctness_criteria = {\n",
    "    \"correctness\": \"\"\"\n",
    "Score 1: The answer is completely incorrect or contradicts the ground truth.\n",
    "Score 2: The answer contains major factual errors or misunderstandings of the ground truth.\n",
    "Score 3: The answer is partially correct but contains some factual errors or omissions.\n",
    "Score 4: The answer is mostly correct with only minor inaccuracies or omissions.\n",
    "Score 5: The answer is completely correct and fully aligned with the ground truth.\"\"\"\n",
    "}\n",
    "\n",
    "correctness_score_evaluator = load_evaluator(\n",
    "    evaluator=\"labeled_score_string\",  \n",
    "    criteria=correctness_criteria,   \n",
    "    normalize_by=5,                \n",
    "    llm=ChatOpenAI(model=\"gpt-4o\", temperature=0.0), \n",
    "    prompt=correctness_score_prompt \n",
    ")\n",
    "\n",
    "# 2번째 샘플에 대해 평가 수행\n",
    "correctness_score_result = correctness_score_evaluator.evaluate_strings(\n",
    "    input=question,                   # 평가에 고려할 내용: 질문\n",
    "    prediction=groq_prediction,       # 평가 대상: LLM 모델의 예측\n",
    "    reference=ground_truth,           # 평가 기준: 정답\n",
    ")\n",
    "\n",
    "print(f'Correctness Score: {correctness_score_result[\"score\"]}')\n",
    "print(f'Correctness Reasoning: {correctness_score_result[\"reasoning\"]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
